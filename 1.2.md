## Spark作业和API
在本节中，我们将简要介绍Apache Spark作业和API。这为Spark2.0架构的后续部分提供了必要的基础。

### 1.2.1 执行过程
任何Spark应用程序都会分离主节点上的单个驱动进程(可以包含多个作业)，然后将执行进程（包含多个任务）分配给多个工作节点。
驱动进程会确定任务进程的数量和组成，这些任务进程是根据为指定作业生成的图形分配给执行节点的。注意，任何工作节点都可以来自多个不同作业的多个任务。
Spark作业与一系列对象依赖相关联，这些依赖关系是以有向无环图（DAG）的方式组织的。例如从Spark UI生成的以下示例。基于这些，Spark可以优化调度（例如确定所需的任务和工作节点的数量）并执行这些任务。

### 1.2.2 弹性分布式数据集

